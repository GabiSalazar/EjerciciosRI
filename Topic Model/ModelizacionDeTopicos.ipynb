{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6175040f-dfc8-4a81-ba21-61a25a77eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import nltk  \n",
    "from gensim.models import Word2Vec  \n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np  \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063d8821-bfec-426c-8065-7f9319a6bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88170108-ac2b-4bd9-9e39-a02d525b73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV\n",
    "#file_path = '/kaggle/input/lex-fridman-podcast-transcript/podcastdata_dataset.csv'\n",
    "file_path = 'podcastdata_dataset.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7284c89e-5bfd-4eef-97b0-4859144e2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para contar palabras y oraciones\n",
    "def count_words(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    return len(text.split())\n",
    "\n",
    "def count_sentences(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    return len(text.split('.'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4c3c21-0da5-4210-9c97-e2308cd04ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               text  Word Count  \\\n",
      "0      1  As part of MIT course 6S099, Artificial Genera...       13424   \n",
      "1      2  As part of MIT course 6S099 on artificial gene...       10217   \n",
      "2      3  You've studied the human mind, cognition, lang...        5989   \n",
      "3      4  What difference between biological neural netw...        5993   \n",
      "4      5  The following is a conversation with Vladimir ...        6374   \n",
      "..   ...                                                ...         ...   \n",
      "314  321  By the time he gets to 2045, we'll be able to ...       12807   \n",
      "315  322  there's a broader question here, right? As we ...       26034   \n",
      "316  323  Once this whole thing falls apart and we are c...       25255   \n",
      "317  324  you could be the seventh best player in the wh...       29911   \n",
      "318  325  turns out that if you train a planarian and th...       33714   \n",
      "\n",
      "     Sentence Count  \n",
      "0               611  \n",
      "1               499  \n",
      "2               292  \n",
      "3               311  \n",
      "4               514  \n",
      "..              ...  \n",
      "314             860  \n",
      "315            1724  \n",
      "316            1802  \n",
      "317            1658  \n",
      "318            1731  \n",
      "\n",
      "[319 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Añadir columnas para número de palabras y oraciones\n",
    "df['Word Count'] = df['text'].apply(count_words)\n",
    "df['Sentence Count'] = df['text'].apply(count_sentences)\n",
    "\n",
    "result = df[['id', 'text', 'Word Count', 'Sentence Count']]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d884c0-4960-437b-b56f-a6c163a00b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las oraciones y los mapeos\n",
    "all_sentences = []  \n",
    "ep_sentence_map = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd062ce4-8151-43e3-a798-cee07176ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrer cada fila del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    episode_id = row['id']  # ID del episodio\n",
    "    text = str(row['text']) if pd.notna(row['text']) else \"\"  \n",
    "    sentences = nltk.sent_tokenize(text)  # Dividir el texto en oraciones\n",
    "\n",
    "    # Recorrer cada oración y asignar un ID de oración dentro del episodio\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        all_sentences.append(nltk.word_tokenize(sentence.lower()))  \n",
    "        ep_sentence_map.append({'ep_id': episode_id, 'st_id': i + 1, 'text': sentence})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ac4c9a-61fd-4303-b47c-5a2707289664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo Word2Vec con todas las oraciones\n",
    "word2vec_model = Word2Vec(sentences=all_sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe0b8ca-ace4-46cc-9cfa-dc701b5233c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar el embedding de una oración\n",
    "def sentence_embedding(sentence):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]  \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(100)  \n",
    "    return np.mean(word_vectors, axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f25be18c-d60e-487d-986a-56a50e2cfc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ep_id  st_id                                               text  \\\n",
      "0           1      1  As part of MIT course 6S099, Artificial Genera...   \n",
      "1           1      2                     He is a professor here at MIT.   \n",
      "2           1      3  He's a physicist, spent a large part of his ca...   \n",
      "3           1      4  But he's also studied and delved into the bene...   \n",
      "4           1      5  Amongst many other things, he is the cofounder...   \n",
      "...       ...    ...                                                ...   \n",
      "443537    325   2085                                Is it in the cells?   \n",
      "443538    325   2086  There are many, many layers to this as always ...   \n",
      "443539    325   2087                    So there are chemical networks.   \n",
      "443540    325   2088   So for example, gene regulatory networks, right?   \n",
      "443541    325   2089   Which, or basically any kind of chemical pathway   \n",
      "\n",
      "                                                embedding  \n",
      "0       [0.3643413186073303, -0.3646314740180969, 0.44...  \n",
      "1       [0.5848942995071411, -0.950747013092041, -0.06...  \n",
      "2       [0.08372806012630463, -0.437065988779068, 0.14...  \n",
      "3       [0.01990199275314808, -0.019194915890693665, 0...  \n",
      "4       [0.1081104502081871, -0.24043169617652893, 0.4...  \n",
      "...                                                   ...  \n",
      "443537  [-0.12001857161521912, -1.1280630826950073, -0...  \n",
      "443538  [0.09407062083482742, -0.5905028581619263, 0.0...  \n",
      "443539  [-0.07371523231267929, -0.41761985421180725, 0...  \n",
      "443540  [0.5273065567016602, -0.25027331709861755, -0....  \n",
      "443541  [-0.1822022646665573, -0.7955846190452576, 0.2...  \n",
      "\n",
      "[443542 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear el nuevo DataFrame con las columnas ep_id, st_id, text, embedding\n",
    "data = []\n",
    "\n",
    "for row in ep_sentence_map:\n",
    "    embedding = sentence_embedding(row['text'])  \n",
    "    data.append({\n",
    "        'ep_id': row['ep_id'],  \n",
    "        'st_id': row['st_id'],  \n",
    "        'text': row['text'],  \n",
    "        'embedding': embedding.tolist()  \n",
    "    })\n",
    "\n",
    "embedding_df = pd.DataFrame(data)  \n",
    "print(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623724ae-adc5-47a0-a6eb-6e91d736dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para medir la similitud de la consulta (query) con las oraciones\n",
    "def get_similar_sentences(query, embedding_df, threshold=0.8):\n",
    "    query_embedding = sentence_embedding(query)  \n",
    "    embedding_df['similarity'] = embedding_df['embedding'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    similar_sentences = embedding_df[embedding_df['similarity'] >= threshold]  \n",
    "    return similar_sentences[['ep_id', 'st_id', 'text', 'similarity']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "701c9e11-7f03-4444-8da0-6615a617a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ep_id  st_id                                               text  \\\n",
      "1997        4    333                   GANs and reinforcement learning.   \n",
      "2237        5    214                   It is standard machine learning.   \n",
      "39052      54    716                           That's machine learning.   \n",
      "53154      70   1120                                  Machine learning.   \n",
      "53155      70   1121                                  Machine learning.   \n",
      "61408      81    869  Is it the success of machine learning and rein...   \n",
      "72598      93    172  Yeah, and in the case of machine learning is a...   \n",
      "73566      94    633                            Reinforcement learning.   \n",
      "74451      95    436                       Rework Deep Learning Summit.   \n",
      "86560     108    222  What's the role of simulation in reinforcement...   \n",
      "87009     108    671  So reinforcement learning can be viewed as a g...   \n",
      "87010     108    672  You can certainly cast supervised learning as ...   \n",
      "124609    132   1991                               So machine learning.   \n",
      "150950    148   1703  You started with reinforcement learning or mac...   \n",
      "155731    151    258      Machine, yeah, machine learning for niceness.   \n",
      "157908    153    531                             With machine learning.   \n",
      "197767    177   1041                Machine learning, broadly speaking.   \n",
      "197768    177   1042            Maybe even supervised machine learning.   \n",
      "259788    216   1056                           Machine learning people.   \n",
      "437187    322   1906            Arranged by machine learning algorithm.   \n",
      "\n",
      "        similarity  \n",
      "1997      0.813086  \n",
      "2237      0.816340  \n",
      "39052     0.852385  \n",
      "53154     0.941854  \n",
      "53155     0.941854  \n",
      "61408     0.822208  \n",
      "72598     0.808268  \n",
      "73566     0.882806  \n",
      "74451     0.813628  \n",
      "86560     0.835875  \n",
      "87009     0.814311  \n",
      "87010     0.803608  \n",
      "124609    0.900804  \n",
      "150950    0.837524  \n",
      "155731    0.811952  \n",
      "157908    0.801075  \n",
      "197767    0.856320  \n",
      "197768    0.831868  \n",
      "259788    0.809754  \n",
      "437187    0.814882  \n"
     ]
    }
   ],
   "source": [
    "query = \"machine learning inventor\"\n",
    "similar_sentences = get_similar_sentences(query, embedding_df)\n",
    "print(similar_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ff36eb-47a6-4cd1-a084-3c792a55ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener lista de episodios únicos\n",
    "episode_ids = embedding_df['ep_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76439f43-3029-4ce3-9754-2a18a4e73e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Evitar el memory leak en Windows\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Limitar a un solo hilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85110d1c-ea6c-474e-a9e1-9d4e92b1f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear lista para almacenar datos de todos los tópicos\n",
    "all_topic_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999779c7-6645-41cc-91f9-e392e84dd097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrer cada episodio\n",
    "for ep_id in episode_ids:\n",
    "    # Filtrar las oraciones del episodio actual\n",
    "    episode_embeddings = embedding_df[embedding_df['ep_id'] == ep_id].copy()  # Copia explícita para evitar SettingWithCopyWarning\n",
    "    embeddings = np.array(episode_embeddings['embedding'].tolist())\n",
    "\n",
    "    # Clustering con KMeans para modelado de tópicos\n",
    "    n_topics = 5  # Número de tópicos estimado\n",
    "    kmeans = KMeans(n_clusters=n_topics, random_state=0, n_init=10)  # Evitar múltiples hilos\n",
    "    episode_embeddings['topic'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # Generación de embeddings de tópicos\n",
    "    for topic_id in episode_embeddings['topic'].unique():\n",
    "        topic_sentences = episode_embeddings[episode_embeddings['topic'] == topic_id]\n",
    "        \n",
    "        # Concatenar todas las oraciones del tópico y calcular el embedding\n",
    "        combined_text = \" \".join(topic_sentences['text'].values)\n",
    "        topic_embedding = sentence_embedding(combined_text)\n",
    "\n",
    "        all_topic_data.append({\n",
    "            'ep_id': ep_id,  # ID del episodio actual\n",
    "            'topic': topic_id,  # ID del tópico\n",
    "            'embedding': topic_embedding.tolist()  # Embedding del tópico\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb81bd7-d67e-474a-bc94-3f6da19d654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con los tópicos de todos los episodios\n",
    "all_topics_df = pd.DataFrame(all_topic_data)\n",
    "print(\"DataFrame de tópicos para todos los episodios:\")\n",
    "print(all_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aece0e-f5a6-459f-819a-22b748b3b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en un archivo CSV\n",
    "all_topics_df.to_csv('all_episodes_topics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486ed03-af7e-4453-a281-c56fc7ebf630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbccc3-a966-452c-bb22-79173d3630b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
