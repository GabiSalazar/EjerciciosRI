{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9876573,"sourceType":"datasetVersion","datasetId":6063605}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ff698db4c87d7582","cell_type":"markdown","source":"# Ejercicio 04: Evaluación de un Sistema de Recuperación de Información\n\n\n\nEl objetivo de este ejercicio es evaluar la efectividad de un sistema de recuperación de información utilizando métricas como *precisión*, *recall*, *F1-score*, *Mean Average Precision (MAP)* y *Normalized Discounted Cumulative Gain (nDCG)*.\n\n\n\nSeguirás los siguientes pasos:","metadata":{}},{"id":"a79fed129105d246","cell_type":"markdown","source":"Descripción del Ejercicio","metadata":{}},{"id":"49500ec8-abef-40bd-9ce4-51abb17f04fe","cell_type":"markdown","source":"1. Proporcionar un Conjunto de Datos:\n\n    * Corpus de Documentos: Utiliza el corpus del ejercicio anterior o un nuevo conjunto de documentos.\n\n    * Consultas: Define un conjunto de consultas específicas.\n\n    * Juicios de Relevancia: Proporciona una lista de qué documentos son relevantes para cada consulta.","metadata":{}},{"id":"bdc2723c-cded-4a0f-b2ca-a8aed724df2d","cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport math\nfrom collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:39.147769Z","iopub.execute_input":"2024-11-11T23:03:39.148289Z","iopub.status.idle":"2024-11-11T23:03:39.183515Z","shell.execute_reply.started":"2024-11-11T23:03:39.148219Z","shell.execute_reply":"2024-11-11T23:03:39.182121Z"}},"outputs":[],"execution_count":1},{"id":"165afbc3-749c-46ac-a62c-f666f7c67bfb","cell_type":"code","source":"# Paso 1: Proporcionar un Conjunto de Datos\n\n# Corpus de Documentos: Utiliza el corpus del ejercicio anterior o un nuevo conjunto de documentos.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:39.186461Z","iopub.execute_input":"2024-11-11T23:03:39.186986Z","iopub.status.idle":"2024-11-11T23:03:39.192738Z","shell.execute_reply.started":"2024-11-11T23:03:39.186928Z","shell.execute_reply":"2024-11-11T23:03:39.190896Z"}},"outputs":[],"execution_count":2},{"id":"272e6f5e-09fa-4c20-b877-b956892b9b25","cell_type":"code","source":"# Definir las consultas\nqueries = {\n    1: \"Impacto de la salud mental en el rendimiento académico de los estudiantes universitarios\",\n    2: \"Actividades extracurriculares y bienestar emocional en el campus universitario\",\n    3: \"Estrategias universitarias para reducir el estrés en estudiantes\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:47.960981Z","iopub.execute_input":"2024-11-11T23:03:47.961410Z","iopub.status.idle":"2024-11-11T23:03:47.967156Z","shell.execute_reply.started":"2024-11-11T23:03:47.961369Z","shell.execute_reply":"2024-11-11T23:03:47.965802Z"}},"outputs":[],"execution_count":3},{"id":"5179ea8d-c142-4ef8-b092-8f1263db2afa","cell_type":"code","source":"# Juicios de Relevancia: Proporciona una lista de qué documentos son relevantes para cada consulta.\n# Los juicios de relevancia son las listas de documentos que consideramos relevantes para cada consulta.\nrelevant_docs = {\n    1: {1, 2, 7},  # Relevantes para la consulta 1\n    2: {5, 8, 15},  # Relevantes para la consulta 2\n    3: {4, 7, 9}    # Relevantes para la consulta 3\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:48.320866Z","iopub.execute_input":"2024-11-11T23:03:48.321316Z","iopub.status.idle":"2024-11-11T23:03:48.327693Z","shell.execute_reply.started":"2024-11-11T23:03:48.321271Z","shell.execute_reply":"2024-11-11T23:03:48.326383Z"}},"outputs":[],"execution_count":4},{"id":"3df04385-08df-45fb-a079-187a23a419a2","cell_type":"markdown","source":"2. Calcular Resultados de Búsqueda:\n\n    * Obten los resultados ordenados de dos sistemas de recuperación para cada consulta.","metadata":{}},{"id":"4d0d0283-5e8a-4300-954c-b9586b0cb560","cell_type":"code","source":"# Paso 2: Calcular Resultados de Búsqueda\n\n# Función para procesar el texto y extraer palabras clave\ndef process_text(text):\n    text = text.lower()  # Convertir todo el texto a minúsculas para normalizar\n    import re\n    text = re.sub(r'[^a-záéíóúñü]+', ' ', text)  # Eliminar caracteres no alfanuméricos\n    tokens = text.strip().split()  # Tokenizar el texto en palabras\n    return set(tokens)  # Devolver un conjunto de palabras clave\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:49.061567Z","iopub.execute_input":"2024-11-11T23:03:49.062050Z","iopub.status.idle":"2024-11-11T23:03:49.068972Z","shell.execute_reply.started":"2024-11-11T23:03:49.062001Z","shell.execute_reply":"2024-11-11T23:03:49.067689Z"}},"outputs":[],"execution_count":5},{"id":"cbd529eb-cb03-44b6-9e33-70707eba4244","cell_type":"code","source":"# Leer y parsear el archivo XML del corpus de documentos\ndef parse_corpus(xml_file):\n    tree = ET.parse(xml_file)  # Parsear el archivo XML\n    root = tree.getroot()  # Obtener el nodo raíz del archivo XML\n    corpus = {}  # Diccionario para almacenar los documentos procesados\n    \n    for doc in root.findall('document'):  # Iterar sobre cada documento en el corpus\n        doc_id = int(doc.get('id'))  # Obtener el ID del documento\n        title = doc.find('title').text  # Obtener el título del documento\n        keywords = doc.find('keywords').text  # Obtener las palabras clave del documento\n        author = doc.find('author').text  # Obtener el autor del documento\n        date = doc.find('date').text  # Obtener la fecha de publicación del documento\n        \n        # Procesar las palabras clave\n        keyword_set = process_text(keywords)\n        \n        # Almacenar el documento en el diccionario 'corpus'\n        corpus[doc_id] = {\n            'title': title,\n            'keywords': keyword_set,\n            'author': author,\n            'date': date\n        }\n    \n    return corpus  # Devolver el diccionario con los documentos procesados","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:49.391393Z","iopub.execute_input":"2024-11-11T23:03:49.392246Z","iopub.status.idle":"2024-11-11T23:03:49.401440Z","shell.execute_reply.started":"2024-11-11T23:03:49.392199Z","shell.execute_reply":"2024-11-11T23:03:49.399740Z"}},"outputs":[],"execution_count":6},{"id":"922dd813-8b90-4848-b630-fcd4e1690580","cell_type":"code","source":"# Calcular la similitud de Jaccard\ndef jaccard_similarity(set1, set2):\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union if union != 0 else 0  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:49.680773Z","iopub.execute_input":"2024-11-11T23:03:49.681224Z","iopub.status.idle":"2024-11-11T23:03:49.687981Z","shell.execute_reply.started":"2024-11-11T23:03:49.681175Z","shell.execute_reply":"2024-11-11T23:03:49.686654Z"}},"outputs":[],"execution_count":7},{"id":"9edea103-a0b3-42f2-a2f6-ca6a87cb6b9a","cell_type":"code","source":"# Calcular la similitud de Coseno\ndef cosine_similarity(set1, set2):\n    intersection = len(set1.intersection(set2)) \n    return intersection / (math.sqrt(len(set1)) * math.sqrt(len(set2))) if len(set1) > 0 and len(set2) > 0 else 0  # Calcular y devolver la similitud coseno\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:50.087833Z","iopub.execute_input":"2024-11-11T23:03:50.088775Z","iopub.status.idle":"2024-11-11T23:03:50.095422Z","shell.execute_reply.started":"2024-11-11T23:03:50.088726Z","shell.execute_reply":"2024-11-11T23:03:50.093998Z"}},"outputs":[],"execution_count":8},{"id":"0509852b-7a23-4365-ab0e-6229f217d63b","cell_type":"code","source":"# Función para obtener los resultados ordenados de búsqueda\ndef get_search_results(queries, corpus, similarity_func):\n    results = {}  # Diccionario para almacenar los resultados de búsqueda\n    \n    # Iterar sobre cada consulta\n    for query_id, query in queries.items():\n        query_set = process_text(query)  # Procesar la consulta\n        similarities = []  # Lista para almacenar la similitud con cada documento\n        \n        # Comparar la consulta con cada documento\n        for doc_id, doc in corpus.items():\n            doc_set = doc['keywords']  # Obtener las palabras clave del documento\n            similarity = similarity_func(query_set, doc_set)  # Calcular la similitud\n            similarities.append((doc_id, similarity))  # Almacenar el ID del documento y la similitud\n        \n        # Ordenar los documentos por similitud (de mayor a menor)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        results[query_id] = similarities  # Almacenar los resultados ordenados de búsqueda\n    \n    return results  # Devolver los resultados","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:50.247748Z","iopub.execute_input":"2024-11-11T23:03:50.248156Z","iopub.status.idle":"2024-11-11T23:03:50.257059Z","shell.execute_reply.started":"2024-11-11T23:03:50.248117Z","shell.execute_reply":"2024-11-11T23:03:50.255709Z"}},"outputs":[],"execution_count":9},{"id":"d79f1b00-cb2e-4a3d-93fe-8e0788c07b83","cell_type":"markdown","source":"3. Calcular las Métricas de Evaluación:\n\n    * Calcular las siguientes métricas para cada sistema y consulta:\n\n        * Precisión en el top-k (Prec@k)\n\n        * Recall\n\n        * F1-score\n\n        * Mean Average Precision (MAP)\n\n        * nDCG","metadata":{}},{"id":"c1ec4203-5159-46aa-84e5-1318d4f80b17","cell_type":"code","source":"# Paso 3: Calcular las Métricas de Evaluación\n\n# Calcular la precisión en el top-k\ndef precision_at_k(results, relevant_docs, k):\n    precision = {}  # Diccionario para almacenar la precisión para cada consulta\n    for query_id, docs in results.items():\n        retrieved = [doc_id for doc_id, _ in docs[:k]]  # Obtener los primeros k documentos recuperados\n        relevant = relevant_docs[query_id]  # Obtener los documentos relevantes para la consulta\n        relevant_retrieved = len(set(retrieved).intersection(relevant))  # Contar cuántos documentos relevantes fueron recuperados\n        precision[query_id] = relevant_retrieved / k  # Calcular la precisión y almacenarla\n    return precision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:50.687903Z","iopub.execute_input":"2024-11-11T23:03:50.688313Z","iopub.status.idle":"2024-11-11T23:03:50.696447Z","shell.execute_reply.started":"2024-11-11T23:03:50.688274Z","shell.execute_reply":"2024-11-11T23:03:50.695181Z"}},"outputs":[],"execution_count":10},{"id":"a3d07406-a8a4-476a-b538-1f9109dd4d0f","cell_type":"code","source":"# Calcular el recall\ndef recall(results, relevant_docs, k):\n    recall_scores = {}  # Diccionario para almacenar el recall para cada consulta\n    for query_id, docs in results.items():\n        retrieved = [doc_id for doc_id, _ in docs[:k]]  # Obtener los primeros k documentos recuperados\n        relevant = relevant_docs[query_id]  # Obtener los documentos relevantes para la consulta\n        relevant_retrieved = len(set(retrieved).intersection(relevant))  # Contar cuántos documentos relevantes fueron recuperados\n        recall_scores[query_id] = relevant_retrieved / len(relevant)  # Calcular el recall y almacenarlo\n    return recall_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:50.919140Z","iopub.execute_input":"2024-11-11T23:03:50.919591Z","iopub.status.idle":"2024-11-11T23:03:50.928265Z","shell.execute_reply.started":"2024-11-11T23:03:50.919548Z","shell.execute_reply":"2024-11-11T23:03:50.926773Z"}},"outputs":[],"execution_count":11},{"id":"17143329-e343-4c42-b240-9a6b1b8cc56d","cell_type":"code","source":"# Calcular el F1-score\ndef f1_score(precision, recall):\n    f1_scores = {}  # Diccionario para almacenar el F1-score\n    for query_id in precision.keys():\n        p = precision[query_id]  # Precisión para la consulta\n        r = recall[query_id]  # Recall para la consulta\n        f1_scores[query_id] = (2 * p * r) / (p + r) if (p + r) != 0 else 0  # Calcular el F1-score\n    return f1_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:51.119657Z","iopub.execute_input":"2024-11-11T23:03:51.120119Z","iopub.status.idle":"2024-11-11T23:03:51.127602Z","shell.execute_reply.started":"2024-11-11T23:03:51.120075Z","shell.execute_reply":"2024-11-11T23:03:51.126260Z"}},"outputs":[],"execution_count":12},{"id":"88740e1b-1f72-4800-b465-4f4e6ec521a2","cell_type":"code","source":"def mean_average_precision(results, relevant_docs):\n    map_score = 0  # Inicializar el MAP en 0\n    for query_id, docs in results.items():\n        relevant_retrieved = 0  # Contador de documentos relevantes recuperados\n        average_precision = 0  # Inicializar la precisión promedio para esta consulta\n        for i, (doc_id, _) in enumerate(docs):\n            if doc_id in relevant_docs[query_id]:  # Si el documento es relevante\n                relevant_retrieved += 1  # Incrementar el contador\n                # Calcular la precisión en la posición i (i+1 porque las posiciones empiezan desde 0)\n                average_precision += relevant_retrieved / (i + 1)\n        # Si hay documentos relevantes, calcular la precisión promedio\n        if relevant_retrieved > 0:\n            average_precision /= relevant_retrieved\n        map_score += average_precision  # Sumar la precisión promedio de esta consulta\n\n    # Dividir entre el número de consultas para obtener el MAP final\n    return map_score / len(results)  # MAP promedio para todas las consultas\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:51.343656Z","iopub.execute_input":"2024-11-11T23:03:51.344112Z","iopub.status.idle":"2024-11-11T23:03:51.352952Z","shell.execute_reply.started":"2024-11-11T23:03:51.344069Z","shell.execute_reply":"2024-11-11T23:03:51.351523Z"}},"outputs":[],"execution_count":13},{"id":"e28aa088-0f2c-494d-b9b0-3bf5298a3634","cell_type":"code","source":"# Calcular el nDCG (Normalized Discounted Cumulative Gain)\ndef ndcg(results, relevant_docs, k):\n    def dcg_at_k(docs, relevant):\n        dcg = 0  # Inicializar el DCG\n        for i, (doc_id, _) in enumerate(docs[:k]):  # Iterar sobre los primeros k documentos\n            if doc_id in relevant:  # Si el documento es relevante\n                dcg += 1 / math.log2(i + 2)  # Calcular el DCG con descuento\n        return dcg\n\n    def ideal_dcg(relevant):\n        ideal_relevance = sorted(relevant, reverse=True)  # Relevancia ideal (ordenada de mayor a menor)\n        return dcg_at_k([(doc_id, 1) for doc_id in ideal_relevance], relevant)  # Calcular el DCG ideal\n    \n    ndcg_scores = {}  # Diccionario para almacenar el nDCG\n    for query_id, docs in results.items():\n        dcg = dcg_at_k(docs, relevant_docs[query_id])  # Calcular el DCG de los resultados recuperados\n        ideal = ideal_dcg(relevant_docs[query_id])  # Calcular el DCG ideal\n        ndcg_scores[query_id] = dcg / ideal if ideal > 0 else 0  # Calcular el nDCG y almacenarlo\n    return ndcg_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:51.871573Z","iopub.execute_input":"2024-11-11T23:03:51.872606Z","iopub.status.idle":"2024-11-11T23:03:51.882187Z","shell.execute_reply.started":"2024-11-11T23:03:51.872556Z","shell.execute_reply":"2024-11-11T23:03:51.880873Z"}},"outputs":[],"execution_count":14},{"id":"e50f1e35-b481-4294-9e85-9fad4041f67c","cell_type":"markdown","source":"4. Análisis y Comparación:\n\n    * Comparar los resultados de los dos sistemas utilizando las métricas calculadas.\n\n    * Discutir cuál sistema es más efectivo y por qué.","metadata":{}},{"id":"initial_id","cell_type":"code","source":"# Paso 4: Análisis y Comparación\n\n# Obtener los resultados utilizando Jaccard y Coseno\ncorpus = parse_corpus('../data/03ranking_corpus.xml')  \njaccard_results = get_search_results(queries, corpus, jaccard_similarity)  # Resultados con Jaccard\ncosine_results = get_search_results(queries, corpus, cosine_similarity)  # Resultados con Coseno\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:53.199681Z","iopub.execute_input":"2024-11-11T23:03:53.200117Z","iopub.status.idle":"2024-11-11T23:03:53.213149Z","shell.execute_reply.started":"2024-11-11T23:03:53.200077Z","shell.execute_reply":"2024-11-11T23:03:53.211888Z"}},"outputs":[],"execution_count":15},{"id":"17853dd4-d31d-4f7c-a74a-f0afa145e19b","cell_type":"code","source":"# Calcular las métricas para ambos métodos (Jaccard y Coseno)\nk = 5  # Consideramos el top-5\nprecision_jaccard = precision_at_k(jaccard_results, relevant_docs, k)\nrecall_jaccard = recall(jaccard_results, relevant_docs, k)\nf1_jaccard = f1_score(precision_jaccard, recall_jaccard)\nmap_jaccard = mean_average_precision(jaccard_results, relevant_docs)\nndcg_jaccard = ndcg(jaccard_results, relevant_docs, k)\n\nprecision_cosine = precision_at_k(cosine_results, relevant_docs, k)\nrecall_cosine = recall(cosine_results, relevant_docs, k)\nf1_cosine = f1_score(precision_cosine, recall_cosine)\nmap_cosine = mean_average_precision(cosine_results, relevant_docs)\nndcg_cosine = ndcg(cosine_results, relevant_docs, k)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:53.871557Z","iopub.execute_input":"2024-11-11T23:03:53.872004Z","iopub.status.idle":"2024-11-11T23:03:53.880186Z","shell.execute_reply.started":"2024-11-11T23:03:53.871964Z","shell.execute_reply":"2024-11-11T23:03:53.878834Z"}},"outputs":[],"execution_count":16},{"id":"15bc50e6-811b-4d24-95c6-48f5331ae698","cell_type":"code","source":"# Función para calcular el promedio de las métricas\ndef calculate_average(metrics):\n    return sum(metrics.values()) / len(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:54.921916Z","iopub.execute_input":"2024-11-11T23:03:54.922354Z","iopub.status.idle":"2024-11-11T23:03:54.931699Z","shell.execute_reply.started":"2024-11-11T23:03:54.922311Z","shell.execute_reply":"2024-11-11T23:03:54.930261Z"}},"outputs":[],"execution_count":17},{"id":"31addbf6-0512-4285-b316-7701d7b9fffb","cell_type":"code","source":"# Mostrar las métricas por cada consulta y los promedios\nprint(\"\\nMétricas para Jaccard:\")\nfor query_id in queries:\n    print(f\"Consulta {query_id}:\")\n    print(f\"  Precisión: {precision_jaccard[query_id]}\")\n    print(f\"  Recall: {recall_jaccard[query_id]}\")\n    print(f\"  F1-score: {f1_jaccard[query_id]}\")\n    print(f\"  MAP: {map_jaccard}\")\n    print(f\"  nDCG: {ndcg_jaccard[query_id]}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:56.381771Z","iopub.execute_input":"2024-11-11T23:03:56.382230Z","iopub.status.idle":"2024-11-11T23:03:56.390260Z","shell.execute_reply.started":"2024-11-11T23:03:56.382186Z","shell.execute_reply":"2024-11-11T23:03:56.389134Z"}},"outputs":[{"name":"stdout","text":"\nMétricas para Jaccard:\nConsulta 1:\n  Precisión: 0.4\n  Recall: 0.6666666666666666\n  F1-score: 0.5\n  MAP: 0.5687533078837427\n  nDCG: 0.7653606369886217\n\nConsulta 2:\n  Precisión: 0.6\n  Recall: 1.0\n  F1-score: 0.7499999999999999\n  MAP: 0.5687533078837427\n  nDCG: 0.9060254355346823\n\nConsulta 3:\n  Precisión: 0.2\n  Recall: 0.3333333333333333\n  F1-score: 0.25\n  MAP: 0.5687533078837427\n  nDCG: 0.20210734650054757\n\n","output_type":"stream"}],"execution_count":18},{"id":"428e6ad3-9246-4196-b360-9f793d73b804","cell_type":"code","source":"# Calcular y mostrar el promedio de cada métrica para Jaccard\nprint(\"Promedio de Métricas para Jaccard:\")\nprint(f\"  Promedio Precisión: {calculate_average(precision_jaccard)}\")\nprint(f\"  Promedio Recall: {calculate_average(recall_jaccard)}\")\nprint(f\"  Promedio F1-score: {calculate_average(f1_jaccard)}\")\nprint(f\"  Promedio MAP: {map_jaccard}\")\nprint(f\"  Promedio nDCG: {calculate_average(ndcg_jaccard)}\")\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:56.719631Z","iopub.execute_input":"2024-11-11T23:03:56.720078Z","iopub.status.idle":"2024-11-11T23:03:56.727301Z","shell.execute_reply.started":"2024-11-11T23:03:56.720034Z","shell.execute_reply":"2024-11-11T23:03:56.726130Z"}},"outputs":[{"name":"stdout","text":"Promedio de Métricas para Jaccard:\n  Promedio Precisión: 0.39999999999999997\n  Promedio Recall: 0.6666666666666666\n  Promedio F1-score: 0.5\n  Promedio MAP: 0.5687533078837427\n  Promedio nDCG: 0.6244978063412838\n\n","output_type":"stream"}],"execution_count":19},{"id":"3938494f-1ab5-4eb5-abad-67e521194ca4","cell_type":"code","source":"# Mostrar las métricas para Coseno\nprint(\"\\nMétricas para Coseno:\")\nfor query_id in queries:\n    print(f\"Consulta {query_id}:\")\n    print(f\"  Precisión: {precision_cosine[query_id]}\")\n    print(f\"  Recall: {recall_cosine[query_id]}\")\n    print(f\"  F1-score: {f1_cosine[query_id]}\")\n    print(f\"  MAP: {map_cosine}\")\n    print(f\"  nDCG: {ndcg_cosine[query_id]}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:03:57.831577Z","iopub.execute_input":"2024-11-11T23:03:57.832055Z","iopub.status.idle":"2024-11-11T23:03:57.839989Z","shell.execute_reply.started":"2024-11-11T23:03:57.832011Z","shell.execute_reply":"2024-11-11T23:03:57.838769Z"}},"outputs":[{"name":"stdout","text":"\nMétricas para Coseno:\nConsulta 1:\n  Precisión: 0.4\n  Recall: 0.6666666666666666\n  F1-score: 0.5\n  MAP: 0.5687533078837427\n  nDCG: 0.7653606369886217\n\nConsulta 2:\n  Precisión: 0.6\n  Recall: 1.0\n  F1-score: 0.7499999999999999\n  MAP: 0.5687533078837427\n  nDCG: 0.9060254355346823\n\nConsulta 3:\n  Precisión: 0.2\n  Recall: 0.3333333333333333\n  F1-score: 0.25\n  MAP: 0.5687533078837427\n  nDCG: 0.20210734650054757\n\n","output_type":"stream"}],"execution_count":20},{"id":"dbcecfd1-ae45-4eee-b176-aa50a19f30b3","cell_type":"code","source":"# Calcular y mostrar el promedio de cada métrica para Coseno\nprint(\"Promedio de Métricas para Coseno:\")\nprint(f\"  Promedio Precisión: {calculate_average(precision_cosine)}\")\nprint(f\"  Promedio Recall: {calculate_average(recall_cosine)}\")\nprint(f\"  Promedio F1-score: {calculate_average(f1_cosine)}\")\nprint(f\"  Promedio MAP: {map_cosine}\")\nprint(f\"  Promedio nDCG: {calculate_average(ndcg_cosine)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T23:04:01.931749Z","iopub.execute_input":"2024-11-11T23:04:01.932185Z","iopub.status.idle":"2024-11-11T23:04:01.939683Z","shell.execute_reply.started":"2024-11-11T23:04:01.932145Z","shell.execute_reply":"2024-11-11T23:04:01.938389Z"}},"outputs":[{"name":"stdout","text":"Promedio de Métricas para Coseno:\n  Promedio Precisión: 0.39999999999999997\n  Promedio Recall: 0.6666666666666666\n  Promedio F1-score: 0.5\n  Promedio MAP: 0.5687533078837427\n  Promedio nDCG: 0.6244978063412838\n","output_type":"stream"}],"execution_count":21},{"id":"71cf92b3-3d07-4586-b4b1-760d32c2eb81","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9854b414-ae2b-4f36-8dab-f575e749cb70","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}